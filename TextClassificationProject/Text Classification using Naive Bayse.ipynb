{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as ma\n",
    "import itertools                                            #this is to slice the dictionary to get only max frequecvy values\n",
    "from sklearn import datasets\n",
    "from nltk.corpus import stopwords                           #to get list of stopwords\n",
    "from sklearn import model_selection\n",
    "from nltk.tokenize import word_tokenize                     #used in removing stopwords from data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary,x,clas):                            # it returns the actual probability of input x over class clas\n",
    "    \n",
    "    count=ma.log(dictionary[clas][\"count\"])-ma.log(dictionary[\"total\"])                      # it is probability of(y=class(clas))\n",
    "    features_number=len(dictionary[clas].keys())-2                                           #total number of features\n",
    "    for j in range(features_number):          # calculting the probabilty over each feature the later we will take log() sum of all            \n",
    "        if(x[j]==0):                           #if input x have zero frequency over the feature so its probabiluty will not counted\n",
    "            continue\n",
    "        count_xj_in_feature_j=dictionary[clas][j] +1                        # it is the total frequency of feature j in class->clas\n",
    "        count_clas_ele_in_feature=dictionary[clas][\"Grand_total\"]+ features_number     # it is total number of words in class->clas\n",
    "        p=ma.log(count_xj_in_feature_j)-ma.log(count_clas_ele_in_feature)          #summing all small probabilities of all features\n",
    "        count=count+p\n",
    "    return count                                                                   # returning the probabilty\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlecol(dictionary,x):        #singlecol gives the prediction(output) of single colum at a time\n",
    "    \n",
    "    best_prob=-1000                # giving any value to initialise best_prob\n",
    "    best_cls=-1                    # giving any value to initialise best_cls\n",
    "    classes=dictionary.keys()      #dictionary .keys have all the classes names\n",
    "    val=True\n",
    "    for clas in classes:           # checking probabily on one class at a time \n",
    "        if clas==\"total\":          # total is not a class so ignore it\n",
    "            continue\n",
    "        clas_p=probability(dictionary,x,clas)        # clas_p will have probability of input x for class clas\n",
    "        if(val or clas_p>best_prob):\n",
    "            best_prob=clas_p\n",
    "            best_cls=clas\n",
    "        val=False\n",
    "    return best_cls                 #returns the best_cls with maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(dictionary,xx_test):         # to obtain output list\n",
    "    \n",
    "    lst=[]\n",
    "    for x in xx_test:                    #going through test_data row wise\n",
    "        pred=singlecol(dictionary,x)     # as we get a answer by one column we are appending it to list\n",
    "        lst.append(pred)\n",
    "    return lst\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(xx_train,y_train,features):              # this function is to train algorithm over training data\n",
    "    result={}                                    #we will use dictionary and create nested dictionary where needed\n",
    "    classes=set(y_train)\n",
    "    \n",
    "    for current_class in classes:                # acceccing all classes one by one \n",
    "        result[current_class]={}\n",
    "        result[\"total\"]=len(xx_train)            # it will hold length of entire xx_train set\n",
    "        current_class_rows=(y_train==current_class)            #fetching only current_class colums\n",
    "        x_train_current=xx_train[current_class_rows]           #spliting x_train for only current_class\n",
    "        y_train_current=y_train[current_class_rows]            #spliting y_train for only current_class\n",
    "        result[current_class][\"count\"]=len(x_train_current)   # it will hold count of current_class (it will be used at time of calculating probability)\n",
    "        features_total=xx_train.shape[1]                      #feature size is nothin but the columns of xx_train\n",
    "        a=0\n",
    "        for j in range(len(features)):\n",
    "            result[current_class][j]=(x_train_current[:,j].sum())         #it will hold frequency of feature j\n",
    "            a+=result[current_class][j]\n",
    "        result[current_class][\"Grand_total\"]=a                            #it will hold count of entire words in current_class\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87        37\n",
      "           1       0.45      0.77      0.57        65\n",
      "           2       0.50      0.02      0.04        54\n",
      "           3       0.55      0.74      0.63        76\n",
      "           4       0.57      0.80      0.67        66\n",
      "           5       0.83      0.42      0.56        59\n",
      "           6       0.68      0.86      0.76        50\n",
      "           7       0.80      0.70      0.75        53\n",
      "           8       0.88      0.89      0.88        63\n",
      "           9       0.80      0.86      0.83        57\n",
      "          10       0.98      0.74      0.84        65\n",
      "          11       0.92      0.89      0.90        53\n",
      "          12       0.62      0.61      0.62        57\n",
      "          13       0.89      0.77      0.83        53\n",
      "          14       0.75      0.82      0.78        65\n",
      "          15       0.80      0.90      0.85        73\n",
      "          16       0.85      0.85      0.85        54\n",
      "          17       0.98      0.87      0.92        63\n",
      "          18       0.80      0.86      0.83        37\n",
      "          19       0.75      0.56      0.64        32\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1132\n",
      "   macro avg       0.77      0.74      0.73      1132\n",
      "weighted avg       0.76      0.74      0.73      1132\n",
      "\n",
      "[[30  0  0  0  0  0  0  0  1  0  0  0  0  0  0  4  0  0  1  1]\n",
      " [ 0 50  0  3  3  1  1  2  0  0  0  1  1  0  2  1  0  0  0  0]\n",
      " [ 0 12  1 24  4  4  4  0  0  0  0  0  5  0  0  0  0  0  0  0]\n",
      " [ 0  5  0 56 10  0  2  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  6 53  0  1  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  5  3 25  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  2  0 43  0  0  0  0  0  0  1  1  1  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  4 37  2  0  0  0  3  0  4  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  2  4 56  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0  0  1 49  1  0  1  1  2  0  0  0  0  0]\n",
      " [ 0  1  0  0  1  0  0  1  0 11 48  0  1  0  0  1  0  0  0  1]\n",
      " [ 0  2  0  0  0  0  0  1  0  0  0 47  1  0  1  1  0  0  0  0]\n",
      " [ 0  4  0  5  8  0  2  0  0  0  0  0 35  1  1  0  1  0  0  0]\n",
      " [ 0  2  0  0  2  0  1  1  0  0  0  0  1 41  0  2  2  0  0  1]\n",
      " [ 0  3  0  0  2  0  2  0  2  0  0  0  1  0 53  0  1  0  1  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  1 66  1  0  1  2]\n",
      " [ 0  1  1  0  1  0  0  0  0  1  0  2  1  0  0  0 46  0  1  0]\n",
      " [ 0  0  0  0  1  0  0  0  1  0  0  1  0  0  1  0  0 55  3  1]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  1  2  0  1  0 32  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  1  2  6  1  1  1 18]]\n",
      "\n",
      "\n",
      "---------COMPARISION---------------this classification is due to sklearn library------------\n",
      "0.7049469964664311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80        37\n",
      "           1       0.39      0.63      0.48        65\n",
      "           2       0.40      0.04      0.07        54\n",
      "           3       0.54      0.67      0.60        76\n",
      "           4       0.60      0.76      0.67        66\n",
      "           5       0.50      0.46      0.48        59\n",
      "           6       0.65      0.86      0.74        50\n",
      "           7       0.69      0.72      0.70        53\n",
      "           8       0.68      0.92      0.78        63\n",
      "           9       0.77      0.84      0.81        57\n",
      "          10       0.98      0.74      0.84        65\n",
      "          11       0.98      0.77      0.86        53\n",
      "          12       0.62      0.61      0.62        57\n",
      "          13       0.77      0.81      0.79        53\n",
      "          14       0.91      0.66      0.77        65\n",
      "          15       0.91      0.79      0.85        73\n",
      "          16       0.78      0.80      0.79        54\n",
      "          17       0.96      0.86      0.91        63\n",
      "          18       0.79      0.73      0.76        37\n",
      "          19       0.69      0.56      0.62        32\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1132\n",
      "   macro avg       0.72      0.70      0.70      1132\n",
      "weighted avg       0.72      0.70      0.70      1132\n",
      "\n",
      "[[30  0  0  0  0  0  0  0  1  0  0  0  0  1  0  3  0  0  1  1]\n",
      " [ 0 41  1  4  3  5  1  5  1  1  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0 12  2 22  2  8  4  1  1  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  8  0 51  7  5  2  1  0  0  0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  4  1  5 50  0  2  0  0  0  0  0  3  1  0  0  0  0  0  0]\n",
      " [ 0 23  0  4  2 27  2  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  1  0 43  1  0  0  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  0  2  5 38  3  0  0  0  2  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  1  2 58  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  3  0  1  0  1 48  1  0  0  1  0  0  0  0  0  2]\n",
      " [ 1  0  0  0  2  0  0  2  1 10 48  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  0  0  1  0  2  0  0  0 41  2  0  0  1  3  0  0  0]\n",
      " [ 0  5  0  4  8  1  1  1  0  0  0  0 35  1  1  0  0  0  0  0]\n",
      " [ 1  2  0  0  2  0  0  0  2  0  0  0  2 43  0  0  1  0  0  0]\n",
      " [ 1  4  0  0  1  2  1  0  6  0  0  0  3  2 43  0  1  0  1  0]\n",
      " [ 1  1  0  0  0  0  1  0  4  1  0  0  0  1  0 58  0  1  1  4]\n",
      " [ 0  1  1  0  1  1  1  0  2  1  0  0  1  0  0  0 43  0  1  1]\n",
      " [ 0  0  0  0  1  0  0  0  2  1  0  1  0  1  0  0  1 54  2  0]\n",
      " [ 1  0  0  0  0  1  1  1  1  0  0  0  0  2  0  0  3  0 27  0]\n",
      " [ 3  0  0  0  0  0  0  1  1  0  0  0  0  1  1  2  3  1  1 18]]\n"
     ]
    }
   ],
   "source": [
    "news=datasets.fetch_20newsgroups()                              # loading data from datasets   to a news name dataframe\n",
    "x=news.data\n",
    "y=news.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(x,y,test_size=0.1,random_state=0)         #doing spliting for train and test data\n",
    "\n",
    "len_data=len(x_train)\n",
    "dictionary=dict()                           #in this dictionary we will store frequency of each word from entire dataset by removing stop_words\n",
    "for j in range(len_data):\n",
    "    data=x_train[j]\n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    for word in filtered_sentence:\n",
    "        if word in dictionary:\n",
    "            dictionary[word]+=1\n",
    "        else:\n",
    "            dictionary[word]=1\n",
    "new_dict={}                                               #this is the reverse sorted form of dictionary used above\n",
    "for key,value in sorted(dictionary.items(),key=lambda item: item[1],reverse=True):\n",
    "    new_dict[key]=value\n",
    "a=dict(itertools.islice(new_dict.items(),3000))           # slicing over bigger ditionary to get max  frequency 3000 data only\n",
    "features=[]                                               # features is the list of keys of dictionary (a) \n",
    "for i in a.keys():\n",
    "    features.append(i)\n",
    "    \n",
    "xx_train=np.zeros((len(x_train),len(features)))           #modifing x_train to xx_train which is 2d and have frequency of each word of features \n",
    "for i in range(len(x_train)):\n",
    "    data=x_train[i]\n",
    "    \n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    for j in filtered_sentence:\n",
    "        if j in features:\n",
    "            xx_train[i][features.index(j)]+=1\n",
    "            \n",
    "xx_test=np.zeros((len(x_test),len(features)))             #xx_test is modified from x_test which is 2d now and have frequency format of each feature\n",
    "for i in range(len(x_test)):\n",
    "    data2=x_test[i]\n",
    "    word_tok=word_tokenize(data2)\n",
    "    \n",
    "    fil_sentence = [w for w in word_tok if not w in stop_words]\n",
    "    \n",
    "    for j in fil_sentence:\n",
    "        if j in features:\n",
    "            xx_test[i][features.index(j)]+=1\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "final_dict=fit(xx_train,y_train,features)        #calling fit function over data xx_train and y_train  \n",
    "y_pred=predict(final_dict,xx_test)               # predict will return the output list of classes  (output)\n",
    "    \n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix      #importing these to check correctness of y_pred(output)\n",
    "print(classification_report(y_test,y_pred))                             #it will give precission and recoil data\n",
    "print(confusion_matrix(y_test,y_pred))                      # it will print confusion matrix to show how is the output result\n",
    "       \n",
    "print()\n",
    "print()\n",
    "print('---------COMPARISION---------------this classification is due to sklearn library------------')\n",
    "from sklearn.naive_bayes import MultinomialNB          # now doing the same fit and predict by MultinomialNB library function\n",
    "arg1=MultinomialNB()\n",
    "arg1.fit(xx_train,y_train)\n",
    "y_pred2=arg1.predict(xx_test)\n",
    "print(arg1.score(xx_test,y_test))   # getting a score of 0.72\n",
    "\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))        \n",
    "\n",
    "    \n",
    "\n",
    "# the average of precision using sketch is 0.77 and due to library it is 0.74 which are nearly same so our code is giving the correct output\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
